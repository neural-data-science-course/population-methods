{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "---\n",
    "\n",
    "You can find below the code that was used to generate the activity of place cells on a linear track.\n",
    "Use the code and the decoding procedure you lerned about in the lesson to explore how different features of the data impact our ability to decode position.\n",
    "In particular:\n",
    "\n",
    "A - Try to use different fractions of our data samples. How does the median error change when the the number of available sample gets larger? You do not need to re-generate any data, just randomly sub-sample the data to different fractions.\n",
    "\n",
    "B - How many place cells do we need to reliably decode position? Try to re-do the decoding using only 10 cell, then 20, and so on. How does the median error change? Does it reach an asymptote? (Also in this case, you do not need to re-generate the data, you can just select a random subset of cells each time)\n",
    "\n",
    "C - Generate new data using the code below, changing the firing rate noise (changing the value of the variable `noise firing_rate`). How does this noise impact the decoding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "---\n",
    "In the loop implementation of the decoder, we used `poisson.logpmf(k,mu)` to calculate the log probability of observing $k$ spikes given an average firing rate of $\\mu$. \n",
    "This is mathematically equivalent to `np.log(poisson.pmf(k,mu))`, in which we calculate the probability, and then take the log.\n",
    "\n",
    "\n",
    "Re-run the decoding substituting this expression:\n",
    "\n",
    "```\n",
    "posterior[i] = sum(np.log(poisson.pmf(spikes_count[t_bin,:],firing_rate_maps[:,i]/fps)+pow(1,-15)))\n",
    "```\n",
    "\n",
    "To the line we are using to calculate the posterior.\n",
    "Do you see any difference in the results? What do you think this is due to?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "---\n",
    "A -Compute and plot the ROC curve of the sequence detection method we saw in the lesson. How good of a classifier is it? \n",
    "   Estimate the performacne with the [area under the curve (AOC)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) metric\n",
    "\n",
    "B - Investigate the effect of the noise level on the detection performance by varying the `noise_x_react` and `noise_t_react` parameters.\n",
    "\n",
    "C - What is the effect of the number of cells? For a fixed noise level, use a smaller and smaller subset of the cell population to detect sequences, and plot the performance as a function of the population size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: devise your own sequence detection method\n",
    "---\n",
    "\n",
    "What kind of sequence can our methods detect and what are often missed? Plot a few examples of misclassified sequences and comment on their features.\n",
    "\n",
    "What method you would propose to make sequecne detection better? Just explore whatever ideas come to your mind, evaluate the performance (possibly varying noise levels and population size) and compare what yu find with the method we used in the lesson. Can you find a better one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "track_length = 200. # the length of our linear track (eg in centimeter)\n",
    "average_firing_rate = 5 # the peak firing rate, averaged across the population \n",
    "n_cells = 100 # how many cells we are recording\n",
    "pf_centers = np.random.rand(n_cells) * track_length # the centers of the place fields for all cells drawn randomly with a uniform distribution on the track\n",
    "pf_size = np.random.gamma(10, size=n_cells) # the size (width) of the place fields, drawn randomly from a gamma distribution \n",
    "pf_rate = np.random.exponential(scale=average_firing_rate, size=n_cells) # the peak firing rate for each cell, drawn from an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0., 200.)\n",
    "true_firing_rate_maps = np.zeros((n_cells, len(bins)))\n",
    "for i in range(n_cells):\n",
    "    true_firing_rate_maps[i,:] = pf_rate[i] * np.exp(-((bins-pf_centers[i])**2)/(2*pf_size[i]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TRAJECTORY\n",
    "\n",
    "n_runs = 10\n",
    "use_stops = False\n",
    "av_running_speed = 10 # the average running speed (in cm/s)\n",
    "fps = 10 # the number of \"video frames\" per second \n",
    "running_speed_a = np.random.chisquare(10, size=n_runs) # running speed in the two directions\n",
    "running_speed_b = np.random.chisquare(10, size=n_runs) \n",
    "\n",
    "stopping_time_a = np.random.chisquare(15, size=n_runs) # the time the mouse will spend at the two ends of the track\n",
    "stopping_time_b = np.random.chisquare(15, size=n_runs)\n",
    "\n",
    "x = np.array([])\n",
    "\n",
    "\n",
    "for i in range(n_runs):\n",
    "    stop1 = np.ones((int(stopping_time_a[i]*fps),)) * 0.\n",
    "    run_length = len(bins) * fps / running_speed_a[i]\n",
    "    run1 = np.linspace(0., float(len(bins)-1), int(run_length))\n",
    "    stop2 = np.ones((int(stopping_time_b[i]*fps),)) * (len(bins)-1.)\n",
    "    run_length = len(bins) * fps / running_speed_b[i]\n",
    "    run2 = np.linspace(len(bins)-1., 0., int(run_length))\n",
    "    if use_stops:\n",
    "        x = np.concatenate((x, stop1, run1, stop2, run2))\n",
    "    else:\n",
    "         x = np.concatenate((x, run1, run2))\n",
    "t = np.arange(len(x))/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 10000.\n",
    "t_sampling = np.arange(0, t[-1], 1. / sampling_rate)\n",
    "x_sampling = np.floor(np.interp(t_sampling, t, x))\n",
    "noise_firing_rate = 0.1 # the baseline noise firing rate\n",
    "spikes = []\n",
    "\n",
    "for i in range(n_cells):\n",
    "    inst_rate = true_firing_rate_maps[i,x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "    spikes_loc = np.random.poisson(inst_rate/sampling_rate)\n",
    "    sp = np.argwhere(spikes_loc)\n",
    "    t_sp = t_sampling[sp]\n",
    "    spikes.append(t_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = 'linear_track_data.pickle' # change this name when you save new data\n",
    "\n",
    "out_data = {}\n",
    "out_data['x'] = x\n",
    "out_data['t'] = t\n",
    "out_data['spikes'] = spikes\n",
    "out_data['track_length'] = track_length\n",
    "out_data['fps'] = fps\n",
    "\n",
    "with open('data/'+file_name,'wb') as f:\n",
    "    pickle.dump(out_data,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec757111aa82fc412dab5a41ba1a33fdb6db5c8112df3ff06fec0dbff050b412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
